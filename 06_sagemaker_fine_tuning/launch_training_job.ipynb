{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning Mistral 7b with Amazon SageMaker\n",
    "In this notebook we'll explore how to fine-tune a [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) model with Amazon SageMaker. We'll use the [Hugging Face](https://huggingface.co/) library to download the model and tokenizer, and we'll use the [Amazon SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/) to fine-tune the model on a sample dataset. The goal of this notebook is to cover several key aspects of fine-tuning LLMs including:\n",
    "- Preparing the data for fine-tuning\n",
    "- Obtaining the base model and tokenizer\n",
    "- Configuring a SageMaker training job\n",
    "- Utilizing QLoRA for parameter efficient fine-tuning (PEFT)\n",
    "- Applying supervised fine-tuning methods to train a model\n",
    "- Improving / Aligning the model's outputs with human preferences using Direct Preference Optimization (DPO)\n",
    "\n",
    "We will utilize the [fine-tuning recipes](https://github.com/huggingface/alignment-handbook) provided by Hugging Face that was used to fine-tune the Mistral-7B model to create the [Zephyr-7B-Beta](HuggingFaceH4/zephyr-7b-beta) model.\n",
    "\n",
    "The recipes utilize the [Transformer Reinforcement Learning (TRL)](https://github.com/huggingface/trl) for both supervised fine-tuning and preference alignment and is easy to adapt to other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uq sagemaker\n",
    "%pip install -Uq datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import json\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "import time\n",
    "from pathlib import Path\n",
    "from utils import download_model\n",
    "\n",
    "boto3_session=boto3.session.Session()\n",
    "\n",
    "smr = boto3_session.client(\"sagemaker-runtime\") # sagemaker runtime client for invoking the endpoint\n",
    "sm = boto3_session.client(\"sagemaker\") \n",
    "s3_rsr = boto3_session.resource(\"s3\")\n",
    "role = sagemaker.get_execution_role()  \n",
    "\n",
    "sess = sagemaker.session.Session(boto3_session, sagemaker_client=sm, sagemaker_runtime_client=smr)  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Model\n",
    "First, we'll download the model and tokenizer from the Hugging Face model hub and upload them to our own S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model_path = download_model(\"mistralai/Mistral-7B-v0.1\", \"./Mistral-7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the model has already been uploaded to the S3 bucket. If not, upload it.\n",
    "model_prefix = local_model_path.name\n",
    "\n",
    "if list(s3_rsr.Bucket(bucket).objects.filter(Prefix=model_prefix)) :\n",
    "    print(\"Model already exists on the S3 bucket\")\n",
    "    print(f\"If you want to upload a new model, please delete the existing model from the S3 bucket with the following command: \\n !aws s3 rm --recursive s3://{bucket}/{model_prefix}\")\n",
    "    s3_model_location = f\"s3://{bucket}/{model_prefix}\"\n",
    "else:\n",
    "    s3_model_location = sess.upload_data(path=local_model_path.as_posix(), bucket=bucket, key_prefix=model_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data and upload to S3\n",
    "Next we need to prepare the data for fine-tuning. We can use a sample of the data that was used to train the Zephy-7B-Beta model or we can bring our own data. \n",
    "\n",
    "If bringing our own data, we need to convert it into a json-lines format that is supported by the TRL trainers. \n",
    "- For Supervised Fine-tuning each record should contain a `messages` field. This field should contain a list of dictionaries that correspond to a conversation between a `user` and an AI `assistant`. The schema is `{\"role\": \"{role}\", \"content\": {content}}` where role is either `user`, `assistant`, or `system` and content is the text of the message. For more information see the recipe documentation [here](https://github.com/huggingface/alignment-handbook/tree/main/scripts) or an example dataset [here](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k) \n",
    "- For Direct Preference Optimization we need to provide a dataset that contains `chosen` and `rejected` responses as based on human preference. The schema for this dataset contains `chosen` and `rejected` fields that contain the conversation messages in the same format as the supervised fine-tuning dataset. For more information see the recipe documentation [here](https://github.com/huggingface/alignment-handbook/tree/main/scripts) or an example dataset [here](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized)\n",
    "\n",
    "The tuning recipe will automatically convert the `messages` into a chat prompt that will be used to fine-tune the model. You can see what the default template looks like by visiting the [Zephyr-7B-Beta](HuggingFaceH4/zephyr-7b-beta) model card. The messages will be converted into a chat prompt that separates the system, user, and assistant messages with the following tokens: `<|system|>`, `<|user|>`, and `<|assistant|>` respectively along with an EOS token `</s>` at the end of each message. The template can be adjusted in the tuning script however it is important to keep in mind that the same template should be used during inference and should be well documented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "USE_EXAMPLE_DATA = True # set to False to use your own data\n",
    "NUM_SAMPLES = 1200 # number of samples to use from the example data\n",
    "\n",
    "if USE_EXAMPLE_DATA:\n",
    "    sft_dataset = datasets.load_dataset(\"HuggingFaceH4/ultrachat_200k\")['train_sft'].select(range(NUM_SAMPLES))\n",
    "    dpo_dataset = datasets.load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\")['train_prefs'].select(range(NUM_SAMPLES))\n",
    "    \n",
    "# adjust these values if bringing your own data\n",
    "# In the example here, a jsonl file is stored in /data/dpo and /data/sft that contains data in the format described above\n",
    "else:\n",
    "    dpo_dataset_path =\"./data/dpo\"\n",
    "    sft_dataset_path =\"./data/sft\"\n",
    "    try:\n",
    "        sft_dataset = datasets.load_dataset(sft_dataset_path)[\"train\"]\n",
    "        dpo_dataset = datasets.load_dataset(dpo_dataset_path)[\"train\"]\n",
    "    except Exception as e:\n",
    "        print(\"Please make sure that the data is present in the data folder. If not, please prepare the data first\")\n",
    "        raise Exception(e)\n",
    "\n",
    "sft_dataset.train_test_split(test_size=0.1, shuffle=True, seed=42).save_to_disk('fine-tuning-data/sft_split')\n",
    "dpo_dataset.train_test_split(test_size=0.1, shuffle=True, seed=42).save_to_disk('fine-tuning-data/dpo_split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the data to S3\n",
    "s3_data = sess.upload_data(path=\"fine-tuning-data\", bucket=bucket, key_prefix=\"fine-tuning-mistral/data\")\n",
    "\n",
    "print(f\"Uploaded training data file to {s3_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure SageMaker Training Job for Supervised Fine-tuning\n",
    "Now that the data is ready, we can configure the first SageMaker training job which will perform supervised fine-tuning. The code from the Hugging Face recipe [repo](https://github.com/huggingface/alignment-handbook/tree/main) is cloned into the `src` directory. The `src` directory also contains a requirements.txt file that will install the recipe module and [Flash Attention](https://github.com/Dao-AILab/flash-attention) to speed up the training.\n",
    "\n",
    "The repo contains two scripts, `alignment-handbook/scripts/run_sft.py` for supervised fine-tuning and `alignment-handbook/scripts/run_dpo.py` for direct preference optimization. Both scripts take a positional argument for the path of the recipe file like this `python run_{task}.py config_full.yaml`. The recipe file contains all of the hyperparameters for the training job. The recipe file for the supervised fine-tuning job is located at `src/config_sft_lora.yaml`. Several example recipe files are available within the repo for full and parameter efficient fine-tuning. We will utilize the parameter efficient fine-tuning recipe for this example.\n",
    "\n",
    "A few changes are required to the recipe file to run the job on SageMaker. First, we need to change the `model_name_or_path` to `/opt/ml/input/data/model`. This is the directory to which our base model will be copied to from S3. Next, we need to change the `dataset_mixer` directories to `/opt/ml/input/data/train` which is where our training data will be copied to from S3. Finally, we need to change the `output_dir` for the `trainer` to `/opt/ml/model` so that the model is saved to the `/opt/ml/model` directory which is the default directory for SageMaker models. The contents of the `/opt/ml/model` will be copied to S3 once the job finishes.  Optionally, we can set the `logging_dir` to `/opt/ml/output/tensorboard` to utilize [SageMaker Managed TensorBoard](https://docs.aws.amazon.com/sagemaker/latest/dg/tensorboard-on-sagemaker.html) for monitoring the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.debugger import TensorBoardOutputConfig\n",
    "import time\n",
    "\n",
    "str_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "\n",
    "tb_output_config = TensorBoardOutputConfig(s3_output_path=f\"s3://{bucket}/fine-tuning-mistral/tensorboard/{str_time}\",\n",
    "    container_local_output_path=\"/opt/ml/output/tensorboard\")\n",
    "\n",
    "job_name = f\"mistral7b-sft\"\n",
    "\n",
    "# the default script takes the yaml file as a positional argument\n",
    "# Since sagemaker only supports passing named arguments as hyperparameters, as small change was made to the fine tuning scripts\n",
    "\n",
    "hyperparameters = {\n",
    "    \"recipe\": \"config_sft_lora.yaml\",  # supervised fine-tuning with QLoRA recipe\n",
    "}\n",
    "\n",
    "sft_estimator = PyTorch(\n",
    "    base_job_name=job_name,\n",
    "    source_dir = \"src\",                                  # directory containing the fine-tuning scripts\n",
    "    entry_point=\"alignment-handbook/scripts/run_sft.py\", # fine-tuning script that will be run\n",
    "    sagemaker_session=sess,\n",
    "    role=role,\n",
    "    instance_count=2,                                    # number of instances to use for training \n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_type=\"ml.g5.2xlarge\", \n",
    "    framework_version=\"2.1.0\",                          # PyTorch version\n",
    "    py_version=\"py310\",\n",
    "    disable_profiler=True,\n",
    "    max_run=60*60*24*2,\n",
    "    keep_alive_period_in_seconds=3600,                    # after job is done keep the training cluster alive for 1 hour to accept other jobs\n",
    "    tensorboard_output_config=tb_output_config,\n",
    "    environment = {\"HUGGINGFACE_HUB_CACHE\": \"/tmp\", \n",
    "                    \"LIBRARY_PATH\": \"/opt/conda/lib/\",\n",
    "                    \"TRANSFORMERS_CACHE\": \"/tmp\",\n",
    "                    \"NCCL_P2P_LEVEL\": \"NVL\"},\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}}, # enable distributed training with torch.distributed \n",
    "    disable_output_compression = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoking the fit method on the estimator starts the training job\n",
    "# data will be copied into training cluster based on the dictionary keys specified here\n",
    "# The contents of the s3_model_location will be copied into the /opt/ml/input/data/model directory\n",
    "# The contents of the s3_data will be copied into the /opt/ml/input/data/train directory\n",
    "sft_estimator.fit({\"model\": s3_model_location, \"train\": f\"{s3_data}/sft_split\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure SageMaker Training Job for Direct Preference Optimization\n",
    "Once we have the fine-tuned model, we can further fine-tune it using Direct Preference Optimization to better align with our preferences and improve the model's outputs. The code process is similar to the supervised fine-tuning job. Except now we will use the `alignment-handbook/scripts/run_dpo.py` script and also provide our fine-tuned model as an additional input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "\n",
    "tb_output_config = TensorBoardOutputConfig(s3_output_path=f\"s3://{bucket}/fine-tuning-mistral/tensorboard/{str_time}\",\n",
    "    container_local_output_path=\"/opt/ml/output/tensorboard\")\n",
    "\n",
    "job_name = f\"mistral7b-dpo\"\n",
    "\n",
    "hyperparameters = {\n",
    "    \"recipe\": \"config_dpo_lora.yaml\",\n",
    "}\n",
    "\n",
    "dpo_estimator = PyTorch(\n",
    "    base_job_name=job_name,\n",
    "    source_dir = \"src\",\n",
    "    entry_point=\"alignment-handbook/scripts/run_dpo.py\",\n",
    "    sagemaker_session=sess,\n",
    "    role=role,\n",
    "    instance_count=2, \n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_type=\"ml.g5.2xlarge\", \n",
    "    framework_version=\"2.1.0\",\n",
    "    py_version=\"py310\",\n",
    "    disable_profiler=True,\n",
    "    max_run=60*60*24*2,\n",
    "    keep_alive_period_in_seconds=3600,\n",
    "    tensorboard_output_config=tb_output_config,\n",
    "    environment = {\"HUGGINGFACE_HUB_CACHE\": \"/tmp\", \n",
    "                    \"LIBRARY_PATH\": \"/opt/conda/lib/\",\n",
    "                    \"TRANSFORMERS_CACHE\": \"/tmp\",\n",
    "                    \"NCCL_P2P_LEVEL\": \"NVL\"},\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}},\n",
    "    disable_output_compression = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the location of the fine tuned model from the sft_estimator\n",
    "sft_model_location = sft_estimator.model_data[\"S3DataSource\"][\"S3Uri\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since LoRA was used for fine-tuning, or SFT model will only contain the LoRA adapter. Therefore we also provide the base model as another input into the `.fit` call below. The training script will automatically merge the base model with the LoRA adapter and then proceed with the DPO fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_estimator.fit(\n",
    "    {\n",
    "        \"model\": s3_model_location,       # base Mistral 7B model \n",
    "        \"sft_model\": sft_model_location,  # fine-tuned model from the previous step\n",
    "        \"train\": f\"{s3_data}/dpo_split\",  # preference training data\n",
    "    }\n",
    ")\n",
    "dpo_model_location = dpo_estimator.model_data[\"S3DataSource\"][\"S3Uri\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up\n",
    "Run these cells to remove the data and model artifacts from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 rm --recursive $s3_model_location \n",
    "!aws s3 rm --recursive $sft_model_location\n",
    "!aws s3 rm --recursive $dpo_model_location\n",
    "!aws s3 rm --recursive $s3_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
